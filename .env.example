# ============================================================================
# MODEL PROVIDER SELECTION - AgentCon Demo
# ============================================================================
# Choose ONE of the following by setting the corresponding flag to 'true'
# Priority order: OpenAI > Ollama > Foundry Local

# Option 1: OpenAI API (Recommended for production - highest quality)
USE_OPENAI=false
OPENAI_API_KEY=sk-your-openai-api-key-here
OPENAI_MODEL=gpt-4o-mini

# Option 2: Ollama Local Models (Free - requires local Ollama server)
# Start Ollama server: ollama serve
# Pull model: ollama pull gpt-oss:20b (or any other model: llama2, mistral, etc.)
USE_OLLAMA=true
OLLAMA_BASE_URL=http://localhost:11434/v1
OLLAMA_MODEL=gpt-oss:20b

# Option 3: Microsoft Foundry Local (Enterprise on-premises)
# Requires local Foundry instance running
USE_FOUNDRY_LOCAL=false
LOCAL_BASE_URL=http://localhost:56238/v1
LOCAL_MODEL=gpt-oss-20b-generic-cpu:1

# ============================================================================
# LEGACY CONFIGURATION (for other system components)
# ============================================================================
USE_OPENAI_FALLBACK=true
OPENAI_ORGANIZATION_ID=org-key
OPENAI_CHAT_MODEL_ID=gpt-4o-mini
PROMPT_COST_PER_1M=0.05
COMPLETION_COST_PER_1M=0.40
OPENAI_IMAGE_RECOGNITION_MODEL=gpt-4-vision
OPENAI_IMAGE_RECOGNITION_MODEL_INPUT_COST_PER_1M=0.10
OPENAI_IMAGE_RECOGNITION_MODEL_COMPLETION_COST_PER_1M=0.40
OPENAI_IMAGE_MODEL=gpt-image-1-mini
OPENAI_IMAGE_MODEL_INPUT_COST_PER_1M=2.00
OPENAI_IMAGE_MODEL_COMPLETION_COST_PER_1M=0.20
RATE_LIMIT_PER_MIN=60
LLM_CALL_TIMEOUT=30

# Application Settings
DEBUG=True
LOG_LEVEL=INFO

# ============================================================================
# QUICK START GUIDE
# ============================================================================
# 
# For OpenAI:
#   1. Set USE_OPENAI=true
#   2. Set OPENAI_API_KEY=sk-your-key
#   3. Optionally change OPENAI_MODEL (default: gpt-4o-mini)
#   4. Run: python agentcon_demo.py
#
# For Ollama (Recommended for local development):
#   1. Install Ollama from https://ollama.ai
#   2. Start server: ollama serve
#   3. Pull model: ollama pull gpt-oss:20b
#   4. Set USE_OLLAMA=true (already enabled by default)
#   5. Run: python agentcon_demo.py
#
# For Foundry Local:
#   1. Set USE_FOUNDRY_LOCAL=true
#   2. Set LOCAL_BASE_URL to your Foundry endpoint
#   3. Run: python agentcon_demo.py
#
# ==========================================================================

